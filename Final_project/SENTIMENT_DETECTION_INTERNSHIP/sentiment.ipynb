{"cells":[{"cell_type":"code","execution_count":null,"id":"c4e3c711-5ef9-42b2-a51b-d467822afc02","metadata":{"id":"c4e3c711-5ef9-42b2-a51b-d467822afc02"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from bs4 import BeautifulSoup\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Embedding, LSTM, RNN, Dense\n"]},{"cell_type":"code","execution_count":null,"id":"25d953f4-7b0e-4497-a6c6-9076e022aba1","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":255},"id":"25d953f4-7b0e-4497-a6c6-9076e022aba1","executionInfo":{"status":"error","timestamp":1712292333911,"user_tz":-330,"elapsed":3010,"user":{"displayName":"Sai varun Konda","userId":"17415207119369602422"}},"outputId":"21b4e767-a6db-4084-b7c6-aa273e62296e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n","\u001b[0m"]},{"output_type":"error","ename":"NameError","evalue":"name 're' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-b92aef948755>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mclean_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mclean_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"[^\\w\\s]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Re import needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mclean_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\d+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 're' is not defined"]}],"source":["\n","data = pd.read_csv(\"sent.csv\")\n","\n","data = data.dropna()\n","texts = data[\"clean_text\"]\n","labels = data[\"category\"]\n","\n","processed_texts = []\n","for text in texts:\n","    if not isinstance(text, str):\n","        print(\"Non-string value encountered:\", text)\n","        continue\n","\n","    clean_text = BeautifulSoup(text, \"html.parser\").get_text()\n","\n","    clean_text = re.sub(r\"[^\\w\\s]\", \"\", clean_text)  # Re import needed\n","    clean_text = re.sub(r\"\\d+\", \"\", clean_text)\n","\n","    tokens = nltk.word_tokenize(clean_text)\n","\n","    stop_words = set(stopwords.words(\"english\"))\n","    tokens = [token for token in tokens if token not in stop_words]\n","\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","\n","    processed_texts.append(\" \".join(tokens))\n"]},{"cell_type":"code","execution_count":null,"id":"fe140426-8957-46b2-bda9-32a58fc5d059","metadata":{"id":"fe140426-8957-46b2-bda9-32a58fc5d059"},"outputs":[],"source":["\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(processed_texts)\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","sequences = tokenizer.texts_to_sequences(processed_texts)\n","\n","max_length = max([len(seq) for seq in sequences])\n","padded_sequences = pad_sequences(sequences, maxlen=max_length)\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":315},"id":"bXjsozwSj3ki","executionInfo":{"status":"error","timestamp":1712292224045,"user_tz":-330,"elapsed":27975,"user":{"displayName":"Sai varun Konda","userId":"17415207119369602422"}},"outputId":"6bb6d9e9-1276-4aea-f54c-550f500bd91d"},"id":"bXjsozwSj3ki","execution_count":null,"outputs":[{"output_type":"error","ename":"MessageError","evalue":"Error: credential propagation was unsuccessful","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}]},{"cell_type":"code","execution_count":null,"id":"d7bc97bc-76a1-4daf-bea4-993a6828ef80","metadata":{"id":"d7bc97bc-76a1-4daf-bea4-993a6828ef80"},"outputs":[],"source":["\n","model = Sequential([\n","    Embedding(vocab_size, 100, input_length=max_length),  # Adjust embedding_dim\n","    LSTM(64),  # Or RNN(64)\n","    Dense(3, activation=\"softmax\")  # 3 output units for 3 sentiment classes\n","])\n","\n","model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","\n","from keras.utils import to_categorical\n","\n","\n","one_hot_labels = to_categorical(labels, num_classes=3)  # Assuming 3 classes (-1, 0, 1)\n","\n","model.fit(padded_sequences, one_hot_labels, epochs=10, batch_size=32)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"709d7ba0-8a97-40dd-8fac-065341d973e4","metadata":{"id":"709d7ba0-8a97-40dd-8fac-065341d973e4","outputId":"b58e64df-bdc0-4125-fb47-be2e0b86a9c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["4075/4075 [==============================] - 22s 5ms/step - loss: 0.0089 - accuracy: 0.9974\n","Training Accuracy: 0.9973844885826111\n","1019/1019 [==============================] - 5s 5ms/step - loss: 0.0089 - accuracy: 0.9975\n","Test Accuracy: 0.9974842071533203\n","1019/1019 [==============================] - 5s 5ms/step\n","Confusion Matrix:\n"," [[11098     9     4]\n"," [   34 14405     8]\n"," [   17    10  7009]]\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     11111\n","           1       1.00      1.00      1.00     14447\n","           2       1.00      1.00      1.00      7036\n","\n","    accuracy                           1.00     32594\n","   macro avg       1.00      1.00      1.00     32594\n","weighted avg       1.00      1.00      1.00     32594\n","\n"]}],"source":["from sklearn.model_selection import train_test_split\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Embedding, LSTM, Dense\n","from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(padded_sequences, one_hot_labels, test_size=0.2)  # 20% for testing\n","loss, accuracy = model.evaluate(X_train, y_train)\n","print(\"Training Accuracy:\", accuracy)\n","\n","# Evaluate on the test set\n","test_loss, test_accuracy = model.evaluate(X_test, y_test)\n","print(\"Test Accuracy:\", test_accuracy)\n","\n","# Generate predictions for the test set\n","y_pred = model.predict(X_test)\n","y_pred_classes = np.argmax(y_pred, axis=1)\n","\n","# Analyze performance metrics\n","print(\"Confusion Matrix:\\n\", confusion_matrix(np.argmax(y_test, axis=1), y_pred_classes))\n","print(\"Classification Report:\\n\", classification_report(np.argmax(y_test, axis=1), y_pred_classes))\n"]},{"cell_type":"code","execution_count":null,"id":"c1b0a6bf-3d4e-484c-89b7-8f0252517958","metadata":{"id":"c1b0a6bf-3d4e-484c-89b7-8f0252517958"},"outputs":[],"source":["import pickle\n","\n","with open(\"sent.pkl\", \"wb\") as f:\n","    pickle.dump(model, f)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"c40c7a3f-a74b-4c09-bbb7-b68b49629867","metadata":{"id":"c40c7a3f-a74b-4c09-bbb7-b68b49629867","outputId":"d100c40e-6489-4f55-e594-83097847446f"},"outputs":[{"name":"stdin","output_type":"stream","text":["Enter text (or type 'exit' to quit):  good\n"]},{"name":"stdout","output_type":"stream","text":["good\n","[[34]]\n","(1, 43)\n","1/1 [==============================] - 1s 860ms/step\n","Preprocessed text: good\n","Predicted sentiment: 1\n","good sentence\n"]},{"name":"stdin","output_type":"stream","text":["Enter text (or type 'exit' to quit):  i hate you\n"]},{"name":"stdout","output_type":"stream","text":["hate\n","[[123]]\n","(1, 43)\n","1/1 [==============================] - 1s 956ms/step\n","Preprocessed text: hate\n","Predicted sentiment: 2\n","bad sentence\n"]},{"name":"stdin","output_type":"stream","text":["Enter text (or type 'exit' to quit):  i hate you but also love you\n"]},{"name":"stdout","output_type":"stream","text":["hate also love\n","[[123, 25, 153]]\n","(3, 43)\n","1/1 [==============================] - 1s 858ms/step\n","Preprocessed text: hate also love\n","Predicted sentiment: 0\n","neutral sentence\n"]},{"name":"stdin","output_type":"stream","text":["Enter text (or type 'exit' to quit):  this is a boring life\n"]},{"name":"stdout","output_type":"stream","text":["boring life\n","[[5427, 167]]\n","(2, 43)\n","1/1 [==============================] - 1s 872ms/step\n","Preprocessed text: boring life\n","Predicted sentiment: 2\n","bad sentence\n"]},{"name":"stdin","output_type":"stream","text":["Enter text (or type 'exit' to quit):  a scientific research says all about sky\n"]},{"name":"stdout","output_type":"stream","text":["scientific research say sky\n","[[1505, 999, 14, 1369]]\n","(4, 43)\n","1/1 [==============================] - 1s 872ms/step\n","Preprocessed text: scientific research say sky\n","Predicted sentiment: 0\n","neutral sentence\n"]},{"name":"stdin","output_type":"stream","text":["Enter text (or type 'exit' to quit):  i am thirsty\n"]},{"name":"stdout","output_type":"stream","text":["thirsty\n","[[11372]]\n","(1, 43)\n","1/1 [==============================] - 1s 871ms/step\n","Preprocessed text: thirsty\n","Predicted sentiment: 0\n","neutral sentence\n"]},{"name":"stdin","output_type":"stream","text":["Enter text (or type 'exit' to quit):  exit\n"]}],"source":["import re\n","import nltk\n","\n","from bs4 import BeautifulSoup\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from keras.models import load_model\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","import tensorflow as tf\n","\n","def preprocess_and_predict(text):\n","\n","    clean_text = BeautifulSoup(text, \"html.parser\").get_text()\n","    clean_text = re.sub(r\"[^\\w\\s]\", \"\", clean_text)\n","    clean_text = re.sub(r\"\\d+\", \"\", clean_text)\n","    tokens = nltk.word_tokenize(clean_text)\n","    stop_words = set(stopwords.words(\"english\"))\n","    tokens = [token for token in tokens if token not in stop_words]\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","    preprocessed_text = \" \".join(tokens)\n","    print(preprocessed_text)\n","\n","    with open(\"sent.pkl\", \"rb\") as f:\n","        model = pickle.load(f)\n","\n","    # sequences = tokenizer.texts_to_sequences(preprocessed_text)\n","    # vectorized_text = tokenizer.texts_to_sequences(preprocessed_text)\n","    vectorized_text = tokenizer.texts_to_sequences([tokens])\n","    max_length = max([len(seq) for seq in vectorized_text])\n","    print(vectorized_text)\n","\n","    vectorized_text = pad_sequences([vectorized_text], maxlen=43)\n","    vectorized_text = vectorized_text.reshape( -1, 43)\n","\n","    # vectorized_text = vectorized_text.reshape(None, 43, 100)\n","    # tf.squeeze(vectorized_text, axis=0)\n","\n","    print(vectorized_text.shape)\n","\n","    sentiment_prediction = model.predict(vectorized_text)\n","\n","    print(\"Preprocessed text:\", preprocessed_text)\n","    final_prediction = sentiment_prediction[-1, :].argmax()\n","    print(\"Predicted sentiment:\", final_prediction)\n","    if(final_prediction==0):\n","        print(\"neutral sentence\")\n","    elif(final_prediction==1):\n","        print(\"good sentence\")\n","    elif(final_prediction==2):\n","        print(\"bad sentence\")\n","\n","while True:\n","    user_input = input(\"Enter text (or type 'exit' to quit): \")\n","    if user_input.lower() == \"exit\":\n","        break\n","    preprocess_and_predict(user_input)\n"]},{"cell_type":"code","execution_count":null,"id":"3fb56ade-a54a-4109-b894-124fc4bc4515","metadata":{"id":"3fb56ade-a54a-4109-b894-124fc4bc4515","outputId":"497fc848-60d5-46f3-8816-cf351a4a769d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_9\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_2 (Embedding)     (None, 43, 100)           9654000   \n","                                                                 \n"," lstm_9 (LSTM)               (None, 64)                42240     \n","                                                                 \n"," dense_19 (Dense)            (None, 3)                 195       \n","                                                                 \n","=================================================================\n","Total params: 9696435 (36.99 MB)\n","Trainable params: 9696435 (36.99 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["model.summary()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}